{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from time import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_csv('./data/news_summary.csv', encoding='iso-8859-1')\n",
    "raw = pd.read_csv('./data/news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>headlines</th>\n",
       "      <th>read_more</th>\n",
       "      <th>text</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chhavi Tyagi</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>http://www.hindustantimes.com/india-news/raksh...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daisy Mowke</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>http://www.hindustantimes.com/bollywood/malaik...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arshiya Chopra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>'Virgin' now corrected to 'Unmarried' in IGIMS...</td>\n",
       "      <td>http://www.hindustantimes.com/patna/bihar-igim...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sumedha Sehra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/abu-dujana-...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aarushi Maheshwari</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/sex-traffic...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                  date  \\\n",
       "0        Chhavi Tyagi  03 Aug 2017,Thursday   \n",
       "1         Daisy Mowke  03 Aug 2017,Thursday   \n",
       "2      Arshiya Chopra  03 Aug 2017,Thursday   \n",
       "3       Sumedha Sehra  03 Aug 2017,Thursday   \n",
       "4  Aarushi Maheshwari  03 Aug 2017,Thursday   \n",
       "\n",
       "                                           headlines  \\\n",
       "0  Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1  Malaika slams user who trolled her for 'divorc...   \n",
       "2  'Virgin' now corrected to 'Unmarried' in IGIMS...   \n",
       "3  Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4  Hotel staff to get training to spot signs of s...   \n",
       "\n",
       "                                           read_more  \\\n",
       "0  http://www.hindustantimes.com/india-news/raksh...   \n",
       "1  http://www.hindustantimes.com/bollywood/malaik...   \n",
       "2  http://www.hindustantimes.com/patna/bihar-igim...   \n",
       "3  http://indiatoday.intoday.in/story/abu-dujana-...   \n",
       "4  http://indiatoday.intoday.in/story/sex-traffic...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "\n",
       "                                               ctext  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "      <td>Speaking about the sexual harassment allegatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upGrad learner switches to career in ML & Al w...   \n",
       "1  Delhi techie wins free food from Swiggy for on...   \n",
       "2  New Zealand end Rohit Sharma-led India's 12-ma...   \n",
       "3  Aegon life iTerm insurance plan helps customer...   \n",
       "4  Have known Hirani for yrs, what if MeToo claim...   \n",
       "\n",
       "                                                text  \n",
       "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n",
       "1  Kunal Shah's credit card bill payment platform...  \n",
       "2  New Zealand defeated India by 8 wickets in the...  \n",
       "3  With Aegon Life iTerm Insurance plan, customer...  \n",
       "4  Speaking about the sexual harassment allegatio...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre1 = raw.iloc[:, 0:2].copy()\n",
    "pre2 = summary.iloc[:, 0:6].copy()\n",
    "\n",
    "pre2['text'] = pre2['author'].str.cat(pre2['date'].str.cat(pre2['read_more'].str.cat(pre2['text'].str.cat(pre2['ctext'], sep=' '), sep=' '), sep=' '), sep=' ')\n",
    "\n",
    "pre = pd.DataFrame()\n",
    "pre['text'] = pd.concat([pre1['text'], pre2['text']], ignore_index=True)\n",
    "pre['summary'] = pd.concat([pre1['headlines'], pre2['headlines']],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Speaking about the sexual harassment allegatio...</td>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...   \n",
       "1  Kunal Shah's credit card bill payment platform...   \n",
       "2  New Zealand defeated India by 8 wickets in the...   \n",
       "3  With Aegon Life iTerm Insurance plan, customer...   \n",
       "4  Speaking about the sexual harassment allegatio...   \n",
       "\n",
       "                                             summary  \n",
       "0  upGrad learner switches to career in ML & Al w...  \n",
       "1  Delhi techie wins free food from Swiggy for on...  \n",
       "2  New Zealand end Rohit Sharma-led India's 12-ma...  \n",
       "3  Aegon life iTerm insurance plan helps customer...  \n",
       "4  Have known Hirani for yrs, what if MeToo claim...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_strip(column):\n",
    "    \"\"\"\n",
    "    This function performs data cleansing tasks such as removal of unwanted non-alphanumeric characters, \n",
    "    HTML or hyperlinks\n",
    "    \"\"\"\n",
    "    for row in column:\n",
    "        row = re.sub(\"(\\\\t)\", \" \", str(row)).lower()\n",
    "        row = re.sub(\"(\\\\r)\", \" \", str(row)).lower()\n",
    "        row = re.sub(\"(\\\\n)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove _ if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(__+)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove - if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(--+)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove ~ if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(~~+)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove + if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(\\+\\++)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove . if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(\\.\\.+)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove the characters - <>()|&©ø\"',;?~*!\n",
    "        row = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove mailto:\n",
    "        row = re.sub(\"(mailto:)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove \\x9* in text\n",
    "        row = re.sub(r\"(\\\\x9\\d)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Replace INC nums to INC_NUM\n",
    "        row = re.sub(\"([iI][nN][cC]\\d+)\", \"INC_NUM\", str(row)).lower()\n",
    "\n",
    "        # Replace CM# and CHG# to CM_NUM\n",
    "        row = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", \"CM_NUM\", str(row)).lower()\n",
    "\n",
    "        # Remove punctuations at the end of a word\n",
    "        row = re.sub(\"(\\.\\s+)\", \" \", str(row)).lower()\n",
    "        row = re.sub(\"(\\-\\s+)\", \" \", str(row)).lower()\n",
    "        row = re.sub(\"(\\:\\s+)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Replace any url to only the domain name\n",
    "        try:\n",
    "            url = re.search(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", str(row))\n",
    "            repl_url = url.group(3)\n",
    "            row = re.sub(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", repl_url, str(row))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Remove multiple spaces\n",
    "        row = re.sub(\"(\\s+)\", \" \", str(row)).lower()\n",
    "\n",
    "        # Remove the single character hanging between any two spaces\n",
    "        row = re.sub(\"(\\s+.\\s+)\", \" \", str(row)).lower()\n",
    "\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = text_strip(pre['text'])\n",
    "processed_summary = text_strip(pre['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating input batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner','parser']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text as batches and yield Doc objects in order\n",
    "# text = [str(doc) for doc in nlp.pipe(processed_text, batch_size=5000)]\n",
    "\n",
    "# summary = ['_START_ '+ str(doc) + ' _END_' for doc in nlp.pipe(processed_summary, batch_size=5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list(a_list, file_name):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open(file_name, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "        print('Done writing list into a binary file')\n",
    "\n",
    "# Read list to memory\n",
    "def read_list(file_name):\n",
    "    # for reading also binary mode is important\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_list(text, \"./data/text_pipe\")\n",
    "# write_list(summary, \"./data/summary_pipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_list(\"./data/text_pipe\")\n",
    "summary = read_list(\"./data/summary_pipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saurav kant an alumnus of upgrad and iiit-b pg program in machine learning and artificial intelligence was sr systems engineer at infosys with almost years of work experience the program and upgrad 360-degree career support helped him transition to data scientist at tech mahindra with 90% salary hike upgrad online power learning has powered lakh+ careers.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_START_ upgrad learner switches to career in ml al with 90% salary hike _END_'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre['cleaned_text'] = pd.Series(text)\n",
    "pre['cleaned_summary'] = pd.Series(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102915 entries, 0 to 102914\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   text             102797 non-null  object\n",
      " 1   summary          102915 non-null  object\n",
      " 2   cleaned_text     102915 non-null  object\n",
      " 3   cleaned_summary  102915 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "pre.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_count = []\n",
    "summary_count = []\n",
    "\n",
    "for sent in pre['cleaned_text']:\n",
    "    text_count.append(len(sent.split()))\n",
    "    \n",
    "for sent in pre['cleaned_summary']:\n",
    "    summary_count.append(len(sent.split()))\n",
    "\n",
    "graph_df = pd.DataFrame() \n",
    "\n",
    "graph_df['text'] = text_count\n",
    "graph_df['summary'] = summary_count\n",
    "\n",
    "graph_df.hist(bins = 5)\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Number of Entries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.78389933440218\n"
     ]
    }
   ],
   "source": [
    "#% of text have 0-100 words\n",
    "cnt = 0\n",
    "for i in pre['cleaned_text']:\n",
    "    if len(i.split()) <= 100:\n",
    "        cnt = cnt + 1\n",
    "print(cnt / len(pre['cleaned_text']) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_len = 100\n",
    "max_summary_len = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Plausible Texts and Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saurav kant an alumnus of upgrad and iiit-b pg...</td>\n",
       "      <td>_START_ upgrad learner switches to career in m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kunal shah credit card bill payment platform c...</td>\n",
       "      <td>_START_ delhi techie wins free food from swigg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  saurav kant an alumnus of upgrad and iiit-b pg...   \n",
       "1  kunal shah credit card bill payment platform c...   \n",
       "\n",
       "                                             summary  \n",
       "0  _START_ upgrad learner switches to career in m...  \n",
       "1  _START_ delhi techie wins free food from swigg...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = np.array(pre['cleaned_text'])\n",
    "cleaned_summary= np.array(pre['cleaned_summary'])\n",
    "\n",
    "short_text = []\n",
    "short_summary = []\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if len(cleaned_summary[i].split()) <= max_summary_len and len(cleaned_text[i].split()) <= max_text_len:\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "post_pre = pd.DataFrame({'text': short_text,'summary': short_summary})\n",
    "\n",
    "post_pre.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98348"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98348"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98348"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saurav kant an alumnus of upgrad and iiit-b pg...</td>\n",
       "      <td>sostok _START_ upgrad learner switches to care...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kunal shah credit card bill payment platform c...</td>\n",
       "      <td>sostok _START_ delhi techie wins free food fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new zealand defeated india by wickets in the f...</td>\n",
       "      <td>sostok _START_ new zealand end rohit sharma-le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>with aegon life iterm insurance plan customers...</td>\n",
       "      <td>sostok _START_ aegon life iterm insurance plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>speaking about the sexual harassment allegatio...</td>\n",
       "      <td>sostok _START_ have known hirani for yrs what ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  saurav kant an alumnus of upgrad and iiit-b pg...   \n",
       "1  kunal shah credit card bill payment platform c...   \n",
       "2  new zealand defeated india by wickets in the f...   \n",
       "3  with aegon life iterm insurance plan customers...   \n",
       "4  speaking about the sexual harassment allegatio...   \n",
       "\n",
       "                                             summary  \n",
       "0  sostok _START_ upgrad learner switches to care...  \n",
       "1  sostok _START_ delhi techie wins free food fro...  \n",
       "2  sostok _START_ new zealand end rohit sharma-le...  \n",
       "3  sostok _START_ aegon life iterm insurance plan...  \n",
       "4  sostok _START_ have known hirani for yrs what ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_pre['summary'] = post_pre['summary'].apply(lambda x: 'sostok ' + x \\\n",
    "        + ' eostok')\n",
    "\n",
    "post_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_val, y_tr, y_val = train_test_split(\n",
    "    np.array(post_pre[\"text\"]),\n",
    "    np.array(post_pre[\"summary\"]),\n",
    "    test_size=0.1,\n",
    "    random_state=0,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary:  62.625791318822664\n"
     ]
    }
   ],
   "source": [
    "thresh = 5\n",
    "\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "\n",
    "for key, value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt + 1\n",
    "    if value < thresh:\n",
    "        cnt = cnt + 1\n",
    "    \n",
    "print(\"% of rare words in vocabulary: \", (cnt / tot_cnt) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in X = 29638\n"
     ]
    }
   ],
   "source": [
    "# Prepare a tokenizer, again -- by not considering the rare words\n",
    "x_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "# Convert text sequences to integer sequences \n",
    "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "# Pad zero upto maximum length\n",
    "x_tr = pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "x_voc = x_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in X = {}\".format(x_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 62.55667945587723\n",
      "Size of vocabulary in Y = 12883\n"
     ]
    }
   ],
   "source": [
    "# Prepare a tokenizer on testing data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "thresh = 5\n",
    "\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "\n",
    "for key, value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt + 1\n",
    "    if value < thresh:\n",
    "        cnt = cnt + 1\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
    "\n",
    "# Prepare a tokenizer, again -- by not considering the rare words\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "# Convert text sequences to integer sequences \n",
    "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "# Pad zero upto maximum length\n",
    "y_tr = pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "y_voc = y_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Empty Texts and Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens\n",
    "ind = []\n",
    "\n",
    "for i in range(len(y_tr)):\n",
    "    cnt = 0\n",
    "    for j in y_tr[i]:\n",
    "        if j != 0:\n",
    "            cnt = cnt + 1\n",
    "    if cnt == 2:\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr = np.delete(y_tr, ind, axis=0)\n",
    "x_tr = np.delete(x_tr, ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens\n",
    "ind = []\n",
    "for i in range(len(y_val)):\n",
    "    cnt = 0\n",
    "    for j in y_val[i]:\n",
    "        if j != 0:\n",
    "            cnt = cnt + 1\n",
    "    if cnt == 2:\n",
    "        ind.append(i)\n",
    "\n",
    "y_val = np.delete(y_val, ind, axis=0)\n",
    "x_val = np.delete(x_val, ind, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 200)     5927600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 100, 300), ( 601200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 100, 300), ( 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    2576600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 100, 300), ( 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  601200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 12883)  3877783     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 15,026,783\n",
      "Trainable params: 15,026,783\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 300\n",
    "embedding_dim = 200\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len, ))\n",
    "\n",
    "# Embedding layer\n",
    "enc_emb = Embedding(x_voc, embedding_dim,\n",
    "                    trainable=True)(encoder_inputs)\n",
    "\n",
    "# Encoder Bi-LSTM Forward\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
    "                     return_state=True, dropout=0.2,\n",
    "                     recurrent_dropout=0.2)\n",
    "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
    "\n",
    "# Encoder Bi-LSTM Backward\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
    "                     return_state=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# Encoder LSTM 3\n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
    "                     return_sequences=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_outputs, state_h, state_c) = encoder_lstm3( encoder_output2)\n",
    "\n",
    "# Set up the decoder, using encoder_states as the initial state\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "# Embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Decoder LSTM\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
    "                    return_state=True, dropout=0.4,\n",
    "                    recurrent_dropout=0.2)\n",
    "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
    "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  5/692 [..............................] - ETA: 31:19 - loss: 9.2251"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-e64bdd2ac05c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [x_tr, y_tr[:, :-1]],\n",
    "    y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:],\n",
    "    epochs=50,\n",
    "    callbacks=[es],\n",
    "    batch_size=128,\n",
    "    validation_data=([x_val, y_val[:, :-1]],\n",
    "                     y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:\n",
    "                     , 1:]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('result.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index = y_tokenizer.index_word\n",
    "reverse_source_word_index = x_tokenizer.index_word\n",
    "target_word_index = y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Models\n",
    "\n",
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n",
    "                      state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
    "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n",
    "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
    "                      decoder_state_input_h, decoder_state_input_c],\n",
    "                      [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "\n",
    "    # Encode the input as state vectors.\n",
    "    (e_out, e_h, e_c) = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        (output_tokens, h, c) = decoder_model.predict([target_seq]\n",
    "                + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if sampled_token != 'eostok':\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find the stop word.\n",
    "        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n",
    "            >= max_summary_len - 1:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        (e_h, e_c) = (h, c)\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# To convert sequence to summary\n",
    "def seq2summary(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0 and i != target_word_index['sostok'] and i \\\n",
    "            != target_word_index['eostok']:\n",
    "            newString = newString + reverse_target_word_index[i] + ' '\n",
    "\n",
    "    return newString\n",
    "\n",
    "\n",
    "# To convert sequence to text\n",
    "def seq2text(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0:\n",
    "            newString = newString + reverse_source_word_index[i] + ' '\n",
    "\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 19):\n",
    "    print ('Review:', seq2text(x_tr[i]))\n",
    "    print ('Original summary:', seq2summary(y_tr[i]))\n",
    "    print ('Predicted summary:', decode_sequence(x_tr[i].reshape(1,\n",
    "           max_text_len)))\n",
    "    print '\\n'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
